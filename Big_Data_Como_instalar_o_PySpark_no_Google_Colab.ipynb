{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Big Data: Como instalar o PySpark no Google Colab.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlosfab/sigmoidal_ai/blob/master/Big_Data_Como_instalar_o_PySpark_no_Google_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTruZS5TPEvK",
        "colab_type": "text"
      },
      "source": [
        "# Big Data: Como instalar o PySpark no Google Colab\n",
        "\n",
        "Como instalar o PySpark no Google Colab Ã© uma dÃºvida comum entre aqueles que estÃ£o migrando seus projetos de Data Science para ambientes na nuvem.\n",
        "\n",
        "O termo Big Data estÃ¡ cada vez mais presente, e mesmo projetos pessoais podem assumir uma grande dimensionalidade devido Ã  quantidade de dados disponÃ­veis.\n",
        "\n",
        "Para analisar grandes volumes de dados, Big Data, com velocidade, o Apache Spark Ã© uma ferramenta muito utilizada, dada a sua capacidade de processamento de dados e computaÃ§Ã£o paralela.\n",
        "\n",
        "O Spark foi pensado para ser acessÃ­vel, oferecendo diversas APIs e frameworks em Python, Scala, SQL e diversas outras linguagens.\n",
        "\n",
        "## PySpark no Google Colab\n",
        "\n",
        "PySpark Ã© a interface alto nÃ­vel que permite vocÃª conseguir acessar e usar o Spark por meio da linguagem Python. Usando o PySpark, vocÃª consegue escrever todo o seu cÃ³digo usando apenas o nosso estilo Python de escrever cÃ³digo.\n",
        "\n",
        "JÃ¡ o Google Colab Ã© uma ferramenta incrÃ­vel, poderosa e gratuita â€“ com suporte de GPU inclusive. Uma vez que roda 100% na nuvem, vocÃª nÃ£o tem a necessidade de instalar qualquer coisa na sua prÃ³pria mÃ¡quina.\n",
        "\n",
        "No entanto, apesar da maioria das bibliotecas de Data Science estarem previamente instaladas no Colab, o mesmo nÃ£o acontece com o PySpark. Para conseguir usar o PySpark Ã© necessÃ¡rio alguns passos intermediÃ¡rios, que nÃ£o sÃ£o triviais para aqueles que estÃ£o comeÃ§ando.\n",
        "\n",
        "Dessa maneira, preparei um tutorial simples e direto ensinando a instalar as dependÃªncias e a biblioteca.\n",
        "\n",
        "## Instalando o PySpark no Google Colab\n",
        "\n",
        "Instalar o PySpark nÃ£o Ã© um processo direto como de praxe em Python. NÃ£o basta usar um pip install apenas. Na verdade, antes de tudo Ã© necessÃ¡rio instalar dependÃªncias como o Java 8, Apache Spark 2.3.2 junto com o Hadoop 2.7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz_8sWI7PKEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# instalar as dependÃªncias\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cabkOXVRPYgq",
        "colab_type": "text"
      },
      "source": [
        "A prÃ³xima etapa Ã© configurar as variÃ¡veis de ambiente, pois isso habilita o ambiente do Colab a identificar corretamente onde as dependÃªncias estÃ£o rodando.\n",
        "\n",
        "Para conseguir â€œmanipularâ€ o terminal e interagir como ele, vocÃª pode usar a biblioteca os."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkpG11RQPbRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# configurar as variÃ¡veis de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "\n",
        "# tornar o pyspark \"importÃ¡vel\"\n",
        "import findspark\n",
        "findspark.init('spark-2.4.4-bin-hadoop2.7')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Klur5aiRPdqx",
        "colab_type": "text"
      },
      "source": [
        "Com tudo pronto, vamos rodar uma sessÃ£o local para testar se a instalaÃ§Ã£o funcionou corretamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpJqWggpPKXO",
        "colab_type": "code",
        "outputId": "6b48f3cf-a65c-493d-bed9-bd022afcf56e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "# iniciar uma sessÃ£o local e importar dados do Airbnb\n",
        "from pyspark.sql import SparkSession\n",
        "sc = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "\n",
        "# download do http para arquivo local\n",
        "!wget --quiet --show-progress http://data.insideairbnb.com/brazil/rj/rio-de-janeiro/2019-07-15/visualisations/listings.csv\n",
        "\n",
        "# carregar dados do Airbnb\n",
        "df_spark = sc.read.csv(\"./listings.csv\", inferSchema=True, header=True)\n",
        "\n",
        "# ver algumas informaÃ§Ãµes sobre os tipos de dados de cada coluna\n",
        "df_spark.printSchema()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "listings.csv.1      100%[===================>]   4.49M  6.59MB/s    in 0.7s    \n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- host_id: string (nullable = true)\n",
            " |-- host_name: string (nullable = true)\n",
            " |-- neighbourhood_group: string (nullable = true)\n",
            " |-- neighbourhood: string (nullable = true)\n",
            " |-- latitude: string (nullable = true)\n",
            " |-- longitude: string (nullable = true)\n",
            " |-- room_type: string (nullable = true)\n",
            " |-- price: string (nullable = true)\n",
            " |-- minimum_nights: string (nullable = true)\n",
            " |-- number_of_reviews: string (nullable = true)\n",
            " |-- last_review: string (nullable = true)\n",
            " |-- reviews_per_month: string (nullable = true)\n",
            " |-- calculated_host_listings_count: double (nullable = true)\n",
            " |-- availability_365: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj5eswa9ZzWS",
        "colab_type": "text"
      },
      "source": [
        "## Big Data e Python\n",
        "\n",
        "A biblioteca PySpark permite vocÃª criar seu servidor Apache Spark, trabalhar com grandes volumes de dados e atÃ© mesmo fazer streaming em tempo real.\n",
        "\n",
        "Na minha opiniÃ£o, o Spark Ã© o melhor framework para trabalhar com Big Data. Tenha certeza que o PySpark vai te ajudar muito ao criar uma interface Python que permita a comunicaÃ§Ã£o entre seu projeto e o servidor.\n",
        "\n",
        "Neste artigo, o meu objetivo foi unicamente apresentar a biblioteca, alÃ©m de ensinar como vocÃª pode instalÃ¡-la em um ambiente de nuvem gratuito, o Google Colab. Aproveite e comece a usar hoje mesmo ðŸ™‚"
      ]
    }
  ]
}